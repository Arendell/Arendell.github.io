<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Deepseek on 大飞的避难所🤪</title>
        <link>https://Arendell.github.io/tags/deepseek/</link>
        <description>Recent content in Deepseek on 大飞的避难所🤪</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Arendelle_fei</copyright>
        <lastBuildDate>Mon, 17 Mar 2025 14:10:18 +0800</lastBuildDate><atom:link href="https://Arendell.github.io/tags/deepseek/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>如何在个人机上部署本地大模型</title>
        <link>https://Arendell.github.io/p/%E5%A6%82%E4%BD%95%E5%9C%A8%E4%B8%AA%E4%BA%BA%E6%9C%BA%E4%B8%8A%E9%83%A8%E7%BD%B2%E6%9C%AC%E5%9C%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B/</link>
        <pubDate>Mon, 17 Mar 2025 11:34:35 +0800</pubDate>
        
        <guid>https://Arendell.github.io/p/%E5%A6%82%E4%BD%95%E5%9C%A8%E4%B8%AA%E4%BA%BA%E6%9C%BA%E4%B8%8A%E9%83%A8%E7%BD%B2%E6%9C%AC%E5%9C%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B/</guid>
        <description>&lt;img src="https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20250211135621.png" alt="Featured image of post 如何在个人机上部署本地大模型" /&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;写在前面：&lt;/strong&gt; 本文用于指导对本地部署大模型感兴趣的朋友，方便大家在个人机上体验纯私域大模型，本指导选用全开源软件方案，&lt;font color = red&gt;仅作为抛砖引玉的简单指导性文档，不额外讲解原理介绍、微调模型、训练模型等。&lt;/font&gt;由于部署在个人电脑，因此不额外部署docker，直接部署至windows，如需部署至docker请参考其他官方文档。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;一方案介绍&#34;&gt;一、方案介绍
&lt;/h2&gt;&lt;p&gt;考虑个人机主要为集成显共享显存，整体处理性能较弱，且没有独立CUDA、NPU来加速大模型推理，因此整体采用&lt;font color = red&gt;&lt;strong&gt;1.5b的超微量模型&lt;/strong&gt;&lt;/font&gt;来体验，后端采用&lt;a class=&#34;link&#34; href=&#34;https://ollama.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ollama&lt;/a&gt;挂载开源大模型，前端使用&lt;a class=&#34;link&#34; href=&#34;https://github.com/open-webui/open-webui&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Open-webui&lt;/a&gt;驱动Ollama，方便prompt和对话记录管理。参数量越大（b数值）意味着大模型性能更好，所需硬件成本也是指数级上涨，如电脑硬件更佳，可自行斟酌体验7b、14b、甚至70b、671b的大模型（选取原则及方法见3.1）。&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317114353181.png&#34;
	
	
	
	loading=&#34;lazy&#34;
	
		alt=&#34;整体架构&#34;
	
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;二安装ollama&#34;&gt;二、安装ollama
&lt;/h2&gt;&lt;h3 id=&#34;21-介绍&#34;&gt;2.1 介绍
&lt;/h3&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://ollama.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ollama&lt;/a&gt;是一个开源框架，专门用于在本地机器上便捷部署和运行大型语言模型（LLM）。
Ollama的主要特点和功能包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;简化部署&lt;/strong&gt;：Ollama旨在简化在Docker容器中部署大型语言模型的过程，使得非专业用户也能方便地管理和运行这些复杂的模型。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;轻量级与可扩展&lt;/strong&gt;：作为轻量级框架，Ollama保持较小的资源占用，同时具备良好的可扩展性，允许用户根据需要调整配置以适应不同规模的项目和&lt;font color = red&gt;硬件条件&lt;/font&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;API支持&lt;/strong&gt;：提供了一个&lt;font color = red&gt;简洁的API（能使用open-webUI调度的原因）&lt;/font&gt;，使得开发者能够轻松创建、运行和管理大型语言模型实例，降低了与模型交互的技术门槛。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;预构建模型库&lt;/strong&gt;：包含一系列预先训练好的大型语言模型（&lt;font color = red&gt;纯净的，不需要去别的环境瞎找的大模型）&lt;/font&gt;，用户可以直接选用这些模型应用于自己的应用程序，无需从头训练或自行寻找模型源。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;模型导入与定制&lt;/strong&gt;：支持从特定平台（如GGUF）导入已有的大型语言模型，&lt;font color = red&gt;兼容PyTorch（有微调需求的话兼容PyTorch很方便）&lt;/font&gt;或Safetensors深度学习框架，允许用户将基于这些框架训练的模型集成到Ollama中。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;跨平台支持&lt;/strong&gt;：提供针对macOS、&lt;font color = red&gt;Windows&lt;/font&gt;、Linux以及Docker的安装指南，确保用户能在多种操作系统环境下顺利部署和使用Ollama。
通过这些特点，Ollama使得开发者和研究人员能够在&lt;font color = red&gt;本地环境&lt;/font&gt;中高效利用大型语言模型进行各种自然语言处理任务，而无需依赖云服务或复杂的基础设施设置。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;22-安装步骤&#34;&gt;2.2 安装步骤
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;访问&lt;a class=&#34;link&#34; href=&#34;https://ollama.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Ollama&lt;/a&gt;官网进行软件下载，注意此网站服务器部署于国外，因此下载软件、挂载大模型的过程可能缓慢or超时，请自行查找工具解决。&lt;/li&gt;
&lt;li&gt;点击中间黑色Download按钮，跳转页面中下载Windows版本&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;
&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317130304606.png&#34;  style=&#34;width: 70%; object-fit: contain;&#34; /&gt;
&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317130355953.png&#34;  style=&#34;width: 25%; object-fit: contain;&#34; /&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;安装过程几乎是傻瓜式的，全程只需要点【Install】就好，等右下角显示ollama is running的图标，即代表安装完成。同时打开CMD，键入ollama -v，能正确输出版本即为安装成功。&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317131529001.png&#34; style=&#34;width:50%; object-fit: contain;&#34;/&gt;&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317131814556.png&#34; style=&#34;width:70%; object-fit: contain;&#34;/&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;三挂载大模型&#34;&gt;三、挂载大模型
&lt;/h2&gt;&lt;p&gt;3.1 如何选择大模型&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;继续访问&lt;a class=&#34;link&#34; href=&#34;https://ollama.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ollama的官网&lt;/a&gt;，点击左上角Models即可进入ollama支持的大模型列表&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317132218708.png&#34;/&gt;&lt;/li&gt;
&lt;li&gt;根据所需要的语言及电脑性能，选择合适的大模型，选取原则：
&lt;ul&gt;
&lt;li&gt;Nvidia独立显卡（带有Cuda），根据显存大小选择，基本上1b参数=1G显存，例如6G以下显存的家用Nvidia显卡，选择7b、8b左右的轻量级模型。&lt;/li&gt;
&lt;li&gt;集成显卡，根据性能、内存频宽可选择1.5b、2b、3b的超轻量级模型，处理器性能较强且内存频率&amp;amp;容量够大的，可以选择7b、8b左右的轻量级模型。&lt;/li&gt;
&lt;li&gt;Nvidia旗舰独立显卡、专业计算卡，根据显存大小，可选择32b、70b模型，1b参数≈1G显存。&lt;/li&gt;
&lt;li&gt;阵列专业计算卡、超融合大模型一体机，可选择405b、671b的满血大模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;font color= red&gt;在这里考虑到大家的个人机均为集成显卡的笔记本，笔者下方以1.5b的超轻量级模型为例。&lt;/font&gt;&lt;/p&gt;
&lt;h3 id=&#34;32挂载大模型&#34;&gt;3.2挂载大模型
&lt;/h3&gt;&lt;p&gt;上文除了提到电脑性能，还额外提到了一个关键词&lt;font color= red&gt;【语言】&lt;/font&gt;，由于训练语料不同，不同的大模型支持的语言不尽相同，例如国产对中文支持度最好的大模型是阿里团队的qwen2.5，以及通过qwen2.5蒸馏出的DeepseekR1（以下简称DSR1）:qwen。而使用英文语料训练的llama系列，显然不适用于中文场景。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;DSR1是DSv3的逻辑链增强，因此只有671b的满血版才是真正的DSR1，其他参数量的模型均蒸馏自对应参数量的模型，例如DSR1:7b蒸馏自qwen2.5:7b、DSR1:8b蒸馏自llama3.1:8b，此项内容在ollama的DSR1详情页面处均有标注。可以理解为qwen2.5带了逻辑链的大模型，本身推理能力相较于qwen2.5没有质的飞跃，仅加入了逻辑链推理过程。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;为方便大家顺利的安装至个人机，并体验最新的大模型推理逻辑链技术，笔者选择DSR1:qwen:1.5b的超轻量级模型举例安装。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在ollama的DSR1详情页面，切换模型参数量至1.5b，复制右侧输出栏中的命令：&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317132840377.png&#34;/&gt;&lt;/li&gt;
&lt;li&gt;在cmd中键入对应命令【ollama run deepseek-r1:1.5b】，如报错请参考步骤二重新安装ollama。&lt;/li&gt;
&lt;li&gt;键入后耐心等待ollama拉取对应模型至本地，由于ollama的镜像服务器在国外，此过程如果过慢，请自行寻找工具或挂载新的镜像服务器解决。&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317132903473.png&#34;/&gt;&lt;/li&gt;
&lt;li&gt;耐心等待出现success字样，且出现send a message 字样后，即代表挂载完成。&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317132923162.png&#34;/&gt;&lt;/li&gt;
&lt;li&gt;此状态下由原生Ollama已经可以进行模型推理，我们尝试键入【你好】，得到下列回复，即代表模型推理正常运行。&lt;font color = green&gt;至此完全本地的大模型（DSR1:qwen:1.5b）已正常挂载运行。&lt;/font&gt;&lt;/li&gt;
&lt;li&gt;键入Ctrl+d或/bye可退出ollama的推理模式，更多ollama的命令详情见第五部分。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;四部署open-webui方便对话记录管理大模型调度&#34;&gt;四、部署Open-webui方便对话记录管理&amp;amp;大模型调度
&lt;/h2&gt;&lt;h3 id=&#34;41-介绍&#34;&gt;4.1 介绍
&lt;/h3&gt;&lt;p&gt;Open WebUI是一个可扩展、功能丰富且用户友好的自托管Web用户界面，专为完全离线操作设计。它支持多种大型语言模型(LLM)运行程序，&lt;font color = red&gt;包括Ollama和OpenAI兼容的API。&lt;/font&gt;
Open WebUI的主要特性包括：‌&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;直观的界面&lt;/strong&gt;：聊天界面来自于ChatGPT&lt;font color = red&gt;（使用体验和ChatGPT一致）&lt;/font&gt;，确保了用户友好的体验。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;响应式设计&lt;/strong&gt;：支持流式文字传输&lt;font color = red&gt;（和微信聊天一样）&lt;/font&gt;，提升互动体验。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;代码语法高亮&lt;/strong&gt;：通过语法高亮功能享受增强的代码可读性&lt;font color = red&gt;（支持代码自动识别、嵌入代码块方便阅读复制）&lt;/font&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;全面支持Markdown和LaTeX&lt;/strong&gt;：通过丰富的Markdown和LaTeX功能提升LLM体验&lt;font color = red&gt;（提升阅读体验）&lt;/font&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;本地RAG集成&lt;/strong&gt;：通过检索增强生成模型&lt;font color = red&gt;(RAG)&lt;/font&gt;支持，方便搭建本地知识库。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;网络浏览能力&lt;/strong&gt;：集成&lt;font color = red&gt;网络搜索引擎&lt;/font&gt;，随时将大模型接入网络检索。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;提示预设支持&lt;/strong&gt;：使用聊天输入中的/命令即时访问预设提示，加速互动。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对话标记&lt;/strong&gt;：支持分类和定位特定聊天，以便快速参考和简化数据收集。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;多模型支持&lt;/strong&gt;：&lt;font color = red&gt;无缝切换不同的聊天模型&lt;/font&gt;，根据需求进行多样化的互动。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;42-安装步骤&#34;&gt;4.2 安装步骤
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;访问&lt;a class=&#34;link&#34; href=&#34;https://github.com/open-webui/open-webui&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Open-webui的官方github页面&lt;/a&gt;，或访问&lt;a class=&#34;link&#34; href=&#34;https://docs.openwebui.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;官方docs文档页面&lt;/a&gt;，查看安装教程。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;插一句题外话&lt;/strong&gt;：Open-webui的官方指导是基于docker的，官方本意是基于docker能提升团队项目效率，提升项目高可用度👇，但是本指导仅用于指导【如何在个人机上部署本地大模型】，&lt;font color = red&gt;因此绕过docker直接部署至Windwos是本指导所选用的方案，如需要部署至Docker请参考上方的官方文档。&lt;/font&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;font color = red&gt;安装Python3.11（重要）&lt;/font&gt;，官方文档中明确介绍了open-webui需要Python3.11来作为前置环境，因此我们需要搭建对应的Python环境：&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317133505553.png&#34;/&gt;&lt;/li&gt;
&lt;li&gt;访问&lt;a class=&#34;link&#34; href=&#34;https://www.python.org/downloads/release/python-3110/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Python3.11的官方网站&lt;/a&gt;下载对应版本的Python安装包。&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317133732299.png&#34;/&gt;&lt;/li&gt;
&lt;li&gt;安装点击上方Install Now，整个过程几乎无障碍，为了方便可以点一个添加环境路径，如不会安装Python可自行百度，&lt;font color = green&gt;出现Setup was successful就代表安装完成。&lt;/font&gt;&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317133814698.png&#34; style=&#34;width:50%; object-fit: contain;&#34;/&gt;&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317134049625.png&#34; style=&#34;width:50%; object-fit: contain;&#34;/&gt;&lt;/div&gt;&lt;/li&gt;
&lt;li&gt;此时重新打开CMD，输入python -V（注意V大写），如能正确输出版本为3.11.0即代表Python环境安装完成。&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317134129707.png&#34;/&gt;&lt;/li&gt;
&lt;li&gt;安装Open-webui，有了Python的前置环境，安装Open-webui就简单多了，我们只需要参考官方文档的pip命令，在CMD中键入【pip install open-webui】即可完成安装。因原生pip的镜像站在国外，因此安装过程可能缓慢or超时，请自行查找工具解决。待pip程序运行完成自动退出，即代表安装完成。&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317134553159.png&#34; style=&#34;width:50%; object-fit: contain;&#34;/&gt;&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317134634259.png&#34; style=&#34;width:50%; object-fit: contain;&#34;/&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;43-运行open-webui&#34;&gt;4.3 运行Open-webui
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;安装好后，在cmd中键入【open-webui serve】即可启动Open-webui，当命令行出现Open-WebUI的logo即为启动完成。&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317134809254.png&#34;/&gt;&lt;/li&gt;
&lt;li&gt;浏览器访问127.0.0.1:8080(本机host的8080端口)，即可访问Open-webui的网页,点击【开始探索】后，根据提示逐一创建管理员账号。&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317134830187.png&#34;/&gt;&lt;/li&gt;
&lt;li&gt;登录后耐心等待Open-webui调度ollama挂载的大模型，直到出现chat页面，整个安装过程完成。&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317134848750.png&#34;/&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;五对话llm附录faq&#34;&gt;五、对话LLM&amp;amp;附录FAQ
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;1.5b的DSR1整体逻辑较差，且自动调度逻辑链的机制不明确，为了解决这种问题可以使用(深度思考)前缀来强制调用DSR1的逻辑链模式。&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317134916389.png&#34;/&gt;&lt;/li&gt;
&lt;li&gt;Open-webui默认开启多轮对话，在右侧【高级参数设置】中可以调整输入至大模型的上下文长度，由于众所周知的原因，大模型会随着输入量、对话轮次的增多逐渐变慢，因此&lt;font color = red &gt;不建议进行过长的多轮对话，转而使用左上角的【新对话】创建新的聊天框来沟通。&lt;/font&gt;&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317134934319.png&#34;/&gt;&lt;/li&gt;
&lt;li&gt;点击头像，【管理员设置】如图操作路径，可以开启联网搜索功能，选择开源的【duckduckgo】搜索引擎，点击右下角保存，即可在聊天窗体内嵌入【联网搜索】功能&lt;font color = red &gt;（由于众所周知的原因，联网搜索功能需要自行配置对应的工具才能正常使用）&lt;/font&gt;&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317135421230.png&#34; style=&#34;width:50%; object-fit: contain;&#34;/&gt;&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317135431127.png&#34; style=&#34;width:50%; object-fit: contain;&#34;/&gt;&lt;/div&gt;&lt;br&gt;&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317135526170.png&#34;/&gt;&lt;/li&gt;
&lt;li&gt;ollama命令：基本上和docker命令一样，非常简单。例如可以用ollama ps 查看正在运行的大模型（无调用5分钟左右ollama会自动释放资源，非常人性），利用ollama list可以查看挂载的大模型，同理cp复制、rm删除等&lt;div style=&#34;display: flex; gap: 10px;&#34;&gt;&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317135807401.png&#34; style=&#34;width:40%; object-fit: contain;&#34;/&gt;&lt;img src=&#34;https://arendelle-image-bed.oss-cn-hangzhou.aliyuncs.com/%E5%8D%9A%E5%AE%A2%E5%9B%BE%E5%BA%8A/20250317135431127.png&#34; style=&#34;width:60%; object-fit: contain;&#34;/&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;写在最后&lt;/strong&gt;：自有大模型（chatGPT）以来，我们一直追求大模型的纯私域化、本地化，但根据目前的发展情况来判断，&lt;font color = red &gt;满血大模型本地化部署的成本仍然极高&lt;/font&gt;，而在消费级终端产品上可部署的1.5b、3b、7b的轻量级模型，整体性能较差（比较傻），仅可作为研究和探索使用，不具有明确的生产力。希望各位同事通过本指导能顺利进行大模型的本地化部署，并通过实践深入的理解大模型（Transformer）原理，学会利用大模型做大模型擅长的事，不陷入AI焦虑中。&lt;/p&gt;&lt;/blockquote&gt;
</description>
        </item>
        
    </channel>
</rss>
